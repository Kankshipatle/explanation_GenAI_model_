{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h3O_u4QmWfX",
        "outputId": "26357213-33d1-4c90-886d-045ac426f7c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting explanation_engine.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile explanation_engine.py\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "class ExplanationEngine:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            torch_dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        self.SYSTEM_PROMPT = \"\"\"\n",
        "You are a supply chain explanation assistant.\n",
        "\n",
        "Rules:\n",
        "- Explain using provided facts only\n",
        "- Do not calculate or invent values\n",
        "- Follow the format exactly\n",
        "\n",
        "Format:\n",
        "\n",
        "Cause:\n",
        "<text>\n",
        "\n",
        "Recommendation:\n",
        "<text>\n",
        "\n",
        "Justification:\n",
        "<text>\n",
        "\n",
        "Priority:\n",
        "<High | Medium | Low>\n",
        "\"\"\"\n",
        "\n",
        "    def generate(self, facts: dict) -> str:\n",
        "        prompt = f\"\"\"{self.SYSTEM_PROMPT}\n",
        "\n",
        "FACTS:\n",
        "{facts}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=120,\n",
        "                do_sample=True,\n",
        "                temperature=0.5,\n",
        "                top_p=0.85,\n",
        "                repetition_penalty=1.2,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        gen_tokens = output[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "        return self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from explanation_engine import ExplanationEngine\n",
        "\n",
        "engine = ExplanationEngine()\n",
        "\n",
        "facts = {\n",
        "    \"product_name\": \"Product B\",\n",
        "    \"stock_gap\": 10,\n",
        "    \"demand_change_pct\": 100,\n",
        "    \"lead_time_days\": 7,\n",
        "    \"risk_days\": 3.5,\n",
        "    \"priority\": \"High\"\n",
        "}\n",
        "\n",
        "result = engine.generate(facts)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftMaFy_imbqB",
        "outputId": "a8924c95-fdc5-4fad-837c-27da0a03fdc2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The recommended action is to increase the lead time by 2 days due to a significant demand change of +10%. The product's stock gap has increased from 10 units to 12 units, which will require an additional 4 units to meet the demand. This leads to a total stock shortage of 6 units at the end of week 9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from explanation_engine import ExplanationEngine\n",
        "\n",
        "engine = ExplanationEngine()\n",
        "\n",
        "facts = {\n",
        "    \"product_name\": \"Pen\",\n",
        "    \"stock_gap\": 10,\n",
        "    \"demand_change_pct\": 50,\n",
        "    \"lead_time_days\": 47,\n",
        "    \"risk_days\": 2,\n",
        "    \"priority\": \"High\"\n",
        "}\n",
        "\n",
        "result = engine.generate(facts)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmdlwR3pnJEH",
        "outputId": "5b3bafd4-74d4-4f04-ae62-5912b1d48203"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The demand for pens is expected to increase by 50% in the next month due to an unexpected product launch that will result in higher sales and stock levels. The lead time of 47 days has been reduced as the manufacturer has increased production capacity to meet the increased demand. This reduction in lead time reduces the risk associated with shortages during this period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vIh1p0Gbt8-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}